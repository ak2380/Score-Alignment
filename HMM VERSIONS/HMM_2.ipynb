{"cells":[{"cell_type":"markdown","source":["**HMM Logic Sequence**\n","\n","- Input: MIDI file; audio file\n","- Separate audio into tonal & transient responses\n","- Move through the score sequentially\n","- At each point in the score, take the block of samples\n","- Transition matrix: geometric distribution model of state durations\n","- Emission probability: tonal likelihood using GP LML\n","- Viterbi algorithm 'windowed' to compute most probable state sequence"],"metadata":{"id":"RsOcsgb5hVJ4"}},{"cell_type":"code","source":["pip install pretty_midi pydub"],"metadata":{"id":"06YJpsHa0qsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","from pretty_midi import PrettyMIDI\n","import matplotlib.pyplot as plt\n","\n","\n","def load_and_filter_midi_pretty(file_path, note_range=(60, 72)):\n","    \"\"\"\n","    Loads a MIDI file using pretty_midi and filters it to only include notes within the specified range.\n","\n","    Args:\n","        file_path (str): Path to the MIDI file.\n","        note_range (tuple): A tuple specifying the (min_note, max_note) range.\n","\n","    Returns:\n","        PrettyMIDI: A new PrettyMIDI object with the filtered notes.\n","    \"\"\"\n","    try:\n","        # Load the MIDI file\n","        midi_data = PrettyMIDI(file_path)\n","\n","        # Filter notes for each instrument\n","        for instrument in midi_data.instruments:\n","            filtered_notes = [\n","                note for note in instrument.notes\n","                if note.pitch >= note_range[0] and note.pitch <= note_range[1]\n","            ]\n","            instrument.notes = filtered_notes\n","\n","        return midi_data\n","\n","    except Exception as e:\n","        print(f\"Error loading MIDI file: {e}\")\n","        return None\n","\n","\n","def display_midi_and_notes_pretty(midi_data):\n","    \"\"\"\n","    Displays the notes of a PrettyMIDI object.\n","\n","    Args:\n","        midi_data (PrettyMIDI): The PrettyMIDI object to analyze.\n","\n","    Returns:\n","        list: A list of notes as tuples (start, end, pitch).\n","    \"\"\"\n","    try:\n","        notes = []\n","        for instrument in midi_data.instruments:\n","            for note in instrument.notes:\n","                notes.append((note.start, note.end, note.pitch))\n","\n","        # Sort notes by start time\n","        notes = sorted(notes, key=lambda x: x[0])\n","\n","        def midi_to_note_name(midi_number):\n","            \"\"\"Converts a MIDI note number to a piano note name.\"\"\"\n","            note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n","            octave = (midi_number // 12) - 1\n","            note = note_names[midi_number % 12]\n","            return f\"{note}{octave}\"\n","\n","        # Print and display note names\n","        note_names = [(start, end, midi_to_note_name(pitch)) for start, end, pitch in notes]\n","        print(\"\\nParsed Notes (start, end, pitch):\")\n","        for note in note_names:\n","            print(note)\n","\n","        return notes\n","\n","    except Exception as e:\n","        print(f\"Error displaying MIDI notes: {e}\")\n","        return []\n","\n","\n","def plot_midi_piano_roll_pretty(midi_data):\n","    \"\"\"\n","    Plots a piano roll visualization of a PrettyMIDI object.\n","\n","    Args:\n","        midi_data (PrettyMIDI): The PrettyMIDI object to visualize.\n","    \"\"\"\n","    try:\n","        notes = []\n","        times = []\n","\n","        for instrument in midi_data.instruments:\n","            for note in instrument.notes:\n","                notes.append(note.pitch)\n","                times.append(note.start)\n","\n","        # Plot the piano roll\n","        plt.figure(figsize=(10, 6))\n","        plt.scatter(times, notes, marker='o', color='blue', alpha=0.7)\n","        plt.xlabel(\"Time (seconds)\")\n","        plt.ylabel(\"MIDI Note Number\")\n","        plt.title(\"MIDI Piano Roll Visualization (PrettyMIDI)\")\n","        plt.yticks(range(min(notes), max(notes) + 1, 2))  # Display every 2 notes\n","        plt.grid(True, linestyle='--', alpha=0.6)\n","        plt.show()\n","\n","    except Exception as e:\n","        print(f\"Error plotting piano roll: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mO0Loa-9ixHq","outputId":"504b97e1-2822-424e-a81a-9ec7de2441cf","executionInfo":{"status":"ok","timestamp":1739890206211,"user_tz":0,"elapsed":3652,"user":{"displayName":"Aditya Krishna","userId":"13297794755927845890"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["midi = load_and_filter_midi_pretty('/content/drive/MyDrive/cmajorscale.mid')\n","plot_midi_piano_roll_pretty(midi)"],"metadata":{"id":"MgKNJpmZVC7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_viterbi_output(midi_data, state_sequence, sample_rate=44100, hop_size=hmm_hop_size):\n","    \"\"\"\n","    Plots a piano roll visualization of a PrettyMIDI object with vertical lines at every observation\n","    (every hop_size samples) that are color-coded based on the Viterbi state. Additionally, contiguous\n","    blocks of the same state are labeled at the top with the state number.\n","\n","    Args:\n","        midi_data (pretty_midi.PrettyMIDI): The PrettyMIDI object to visualize.\n","        state_sequence (list or np.array): Viterbi state indices at each observation.\n","        sample_rate (int): Audio sample rate (samples per second).\n","        hop_size (int): Number of samples between observations in state_sequence.\n","    \"\"\"\n","    import numpy as np\n","    import matplotlib.pyplot as plt\n","\n","    try:\n","        notes = []\n","        times = []\n","        for instrument in midi_data.instruments:\n","            for note in instrument.notes:\n","                notes.append(note.pitch)\n","                times.append(note.start)\n","\n","        if not notes:\n","            print(\"No notes found in the MIDI data.\")\n","            return\n","\n","        plt.figure(figsize=(18, 9))\n","        plt.scatter(times, notes, marker='o', color='blue', alpha=0.7)\n","        plt.xlabel(\"Time (seconds)\")\n","        plt.ylabel(\"MIDI Note Number\")\n","        plt.title(\"MIDI Piano Roll Visualization + Viterbi States\")\n","        plt.yticks(range(min(notes), max(notes) + 1, 2))\n","        plt.grid(True, linestyle='--', alpha=0.6)\n","\n","        obs_times = np.arange(len(state_sequence)) * (hop_size / sample_rate)\n","\n","        colors = ['red', 'green', 'blue']\n","        color_idx = 0\n","        if len(state_sequence) > 0:\n","            prev_state = state_sequence[0]\n","            # Draw first vertical line\n","            plt.axvline(x=obs_times[0], color=colors[color_idx], linestyle='--', alpha=0.8)\n","            for i in range(1, len(state_sequence)):\n","                current_state = state_sequence[i]\n","                if current_state != prev_state:\n","                    color_idx = (color_idx + 1) % len(colors)\n","                plt.axvline(x=obs_times[i], color=colors[color_idx], linestyle='--', alpha=0.8)\n","                prev_state = current_state\n","\n","        segments = []\n","        seg_start = 0\n","        for i in range(1, len(state_sequence)):\n","            if state_sequence[i] != state_sequence[i - 1]:\n","                segments.append((seg_start, i - 1, state_sequence[i - 1]))\n","                seg_start = i\n","        segments.append((seg_start, len(state_sequence) - 1, state_sequence[-1]))\n","\n","        ax = plt.gca()\n","        top_y = max(notes) + 2\n","        for seg in segments:\n","            seg_start_idx, seg_end_idx, state = seg\n","            block_start_time = obs_times[seg_start_idx]\n","            block_end_time = obs_times[seg_end_idx] + (hop_size / sample_rate)\n","            mid_time = (block_start_time + block_end_time) / 2.0\n","            ax.text(mid_time, top_y, f\"S={state}\",\n","                    horizontalalignment='center',\n","                    verticalalignment='bottom',\n","                    fontsize=10,\n","                    bbox=dict(facecolor='white', alpha=0.6, edgecolor='none'))\n","\n","        plt.show()\n","\n","    except Exception as e:\n","        print(f\"Error plotting piano roll: {e}\")"],"metadata":{"id":"5Ghgd5Hef07v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import geom\n","import librosa\n","import pretty_midi\n","from scipy.ndimage import median_filter\n","from pydub import AudioSegment\n","import librosa\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from IPython.display import Audio\n","from scipy.io import wavfile\n","from scipy.fft import fft, ifft\n","from scipy.signal.windows import hamming, hann\n","from scipy.linalg import cho_factor, cho_solve\n","\n","\n","\n","\n","audio = AudioSegment.from_file('/content/drive/MyDrive/cmajorscale.mp3')\n","sample_rate = audio.frame_rate\n","audio_data = np.array(audio.get_array_of_samples())\n","\n","time_window_size = 2048\n","hop_size = int(time_window_size * 0.5)\n","hmm_hop_size = 3000\n","window_size = 19\n","alpha = 0.4\n","sigma_p2 = 1e1 # Transient noise\n","\n","sigma_f2 = 2e-5  # Decay parameter for covariance\n","sigma_n2 = 3  # Noise variance\n","wq = [1.0]  # Weights for fundamentals\n","M = 9  # Number of harmonics\n","\n","sample_rate = 44100\n","\n","T = 0.5\n","v = 2.37\n","\n","\n","\n","\n","def parse_midi(midi_data):    #âœ…\n","    notes = []\n","    for instrument in midi_data.instruments:\n","        if not instrument.is_drum:\n","            for note in instrument.notes:\n","                notes.append((note.start, note.end, note.pitch))\n","    notes = sorted(notes, key=lambda x: x[0])\n","    print(notes)\n","    return notes\n","\n","\n","def compute_transition_matrices(E_Z_list, block_size=2048):\n","    \"\"\"\n","    Compute a single left-to-right transition matrix for the entire score.\n","    Each state corresponds to a score event (note/chord) from the MIDI file.\n","\n","    We first compute the average note duration (in samples) from E_Z_list\n","    and then convert it to the average number of blocks (of size 'block_size').\n","    For an average of d_avg blocks per note, a simple duration model is:\n","\n","        self-transition probability:   p = 1 - 1/d_avg\n","        transition to next state:       1 - p\n","\n","    The resulting matrix is of size (n_states x n_states) where n_states is\n","    the number of MIDI events.\n","    \"\"\"\n","    n_states = len(E_Z_list)\n","    avg_duration_samples = np.mean(E_Z_list)\n","    d_avg = avg_duration_samples / block_size\n","\n","    p = 1 - 1/d_avg if d_avg > 1 else 0.0\n","\n","    T = np.zeros((n_states, n_states))\n","    for i in range(n_states):\n","        T[i, i] = p\n","        if i < n_states - 1:\n","            T[i, i+1] = 1 - p\n","\n","    T[n_states-1, n_states-1] = 1\n","    return T\n","\n","\n","\n","def extract_features():   #âœ…\n","    audio = AudioSegment.from_file('/content/drive/MyDrive/cmajorscale.mp3')\n","    sample_rate = audio.frame_rate\n","    samples = np.array(audio.get_array_of_samples())\n","\n","    if audio.channels > 1:   # If the audio is stereo, reshape and convert to mono\n","        samples = samples.reshape((-1, audio.channels))\n","        audio_data = samples.mean(axis=1)\n","    else:\n","        audio_data = samples\n","\n","    time_hamming_window = hann(time_window_size)\n","    randomized_phase = np.random.uniform(-np.pi, np.pi, time_window_size)\n","    processed_audio = np.zeros(len(audio_data))\n","    transient_probabilities = []\n","\n","    time_hamming_window = hann(time_window_size)\n","    randomized_phase = np.random.uniform(-np.pi, np.pi, time_window_size)\n","\n","    processed_audio = np.zeros(len(audio_data))\n","\n","    for start in range(0, len(audio_data) - time_window_size, hop_size):\n","        segment = audio_data[start:start + time_window_size].astype(np.float64)\n","        segment *= time_hamming_window\n","\n","        segment_ft = fft(segment)\n","        magnitude = np.abs(segment_ft)\n","        phase = np.angle(segment_ft)\n","\n","        median_magnitude = alpha * median_filter(magnitude, size=(window_size,))\n","        filtered_magnitude = np.minimum(median_magnitude, magnitude)\n","\n","        filtered_segment_ft = filtered_magnitude * np.exp(1j * phase)\n","        filtered_segment = ifft(filtered_segment_ft).real\n","\n","        processed_audio[start:start + time_window_size] += filtered_segment * time_hamming_window\n","\n","    processed_audio = np.int16(processed_audio / np.max(np.abs(processed_audio)) * 32767)\n","\n","    tonal_audio = audio_data - processed_audio\n","    tonal_audio = np.int16(tonal_audio / np.max(np.abs(tonal_audio)) * 32767)\n","\n","    return tonal_audio, processed_audio\n","\n","\n","def harmonic_weight(m, T, v):   #âœ…\n","    return 1 / (1 + T * m**v)\n","\n","\n","def covariance_function(tau, wq, midi_frequencies, T, v):\n","    cov = np.exp(-2 * np.pi**2 * sigma_f2 * tau**2)\n","    for fq in midi_frequencies:\n","        harmonic_sum = 0\n","        for m in range(1, M + 1):\n","            E_m = harmonic_weight(m, T, v)\n","            harmonic_sum += E_m * np.cos(2 * np.pi * m * fq * tau)\n","        cov += wq[0] * harmonic_sum\n","    return cov\n","\n","\n","def compute_covariance_matrix(midi_frequencies, block_size, sample_rate, T, v):\n","    t = np.linspace(0, block_size / sample_rate, block_size)\n","    tau = t[:, None] - t[None, :]\n","\n","    cov = np.exp(-2 * np.pi**2 * sigma_f2 * tau**2)\n","\n","    harmonic_weights = np.array([harmonic_weight(m, T, v) for m in range(1, M + 1)])\n","    harmonic_weights = harmonic_weights[:, None, None]\n","\n","    for fq in midi_frequencies:\n","        harmonic_matrix = harmonic_weights * np.cos(2 * np.pi * np.arange(1, M + 1)[:, None, None] * fq * tau)\n","        cov += np.sum(harmonic_matrix, axis=0)\n","\n","    return cov\n","\n","\n","def tonal_LML(y, K, sigma_n2):\n","    N = len(y)\n","    K_noise = K + sigma_n2 * np.eye(N)\n","    cho_decomp = cho_factor(K_noise, lower=True)\n","    alpha = cho_solve(cho_decomp, y)\n","    log_det = 2 * np.sum(np.log(np.diag(cho_decomp[0])))\n","    LML = (\n","        -0.5 * np.dot(y.T, alpha)\n","        - 0.5 * log_det\n","        - 0.5 * N * np.log(2 * np.pi)\n","    )\n","    return LML\n","\n","\n","def is_onset_state(state_index, midi_notes, current_time, onset_window=5000/44100):\n","    \"\"\"\n","    Determine if the candidate state is in its transient (onset) phase.\n","    Only returns True if the noteâ€™s onset has occurred (current_time >= onset_time)\n","    and the current time is within onset_window seconds after the onset.\n","    \"\"\"\n","    onset_time = midi_notes[state_index][0]\n","    if current_time < onset_time:\n","        return False\n","    return (current_time - onset_time) < onset_window\n","\n","\n","\n","def compute_emission_probabilities(tonal_audio, transient_audio, sample_rate, midi_notes,\n","                                   block_size=2048, hmm_hop_size=hmm_hop_size,\n","                                   T_val=T, v_val=v, sigma_n2_val=sigma_n2,\n","                                   sigma_t2=0.5):\n","    \"\"\"\n","    Computes state-dependent emission probabilities.\n","    For each observation, we compute probabilities only for a subset of states:\n","      - The previous 4 states (including the maximum allowed state)\n","      - The next 2 states\n","    All other states remain at -infinity.\n","    If a candidate state corresponds to a note onset (detected via transient data),\n","    then the tonal LML is boosted by adding the transient log likelihood.\n","    \"\"\"\n","    n_states = len(midi_notes)\n","    midi_starts = [note[0] for note in midi_notes]\n","\n","    base_frequencies = []\n","    for note in midi_notes:\n","        _, _, pitch = note\n","        base_frequency = 440.0 * 2 ** ((pitch - 69) / 12.0)\n","        base_frequencies.append(base_frequency)\n","\n","    n_obs = (len(tonal_audio) - block_size) // hmm_hop_size + 1\n","    emissions = []\n","    allowed_max_states = []  # one allowed index per observation\n","\n","    for obs_idx, start in enumerate(range(0, len(tonal_audio) - block_size, hmm_hop_size)):\n","        tonal_block = tonal_audio[start:start + block_size]\n","        transient_block = transient_audio[start:start + block_size]\n","        emission_vector = np.full(n_states, -np.inf)\n","\n","        current_time = start / sample_rate\n","        allowed_idx = np.searchsorted(midi_starts, current_time, side='right') - 1\n","        allowed_idx = max(allowed_idx, 0)\n","        allowed_max_states.append(allowed_idx)\n","\n","        lower_bound = max(0, allowed_idx - 3)\n","        upper_bound = min(n_states, allowed_idx + 2 + 1)  # +1 because range() is exclusive at the end\n","\n","        for i in range(lower_bound, upper_bound):\n","            candidate_frequency = base_frequencies[i]\n","\n","            K = compute_covariance_matrix([candidate_frequency], block_size, sample_rate, T_val, v_val)\n","            log_likelihood_tonal = tonal_LML(tonal_block, K, sigma_n2_val)\n","\n","            N_samples = len(transient_block)\n","            log_likelihood_trans = - (\n","                -0.5 * N_samples * np.log(2 * np.pi * sigma_t2)\n","                - 0.5 * np.sum(transient_block.astype(np.float64)**2) / sigma_t2\n","            )\n","\n","            if i == allowed_idx and is_onset_state(i, midi_notes, current_time):\n","                print(f\"Transient detected for state {i} at time {current_time:.2f}s\")\n","                combined_log_likelihood = log_likelihood_tonal + log_likelihood_trans\n","            else:\n","                combined_log_likelihood = log_likelihood_tonal\n","\n","            emission_vector[i] = combined_log_likelihood\n","\n","        print(f\"Block starting at sample {start} (time {current_time:.2f}s): allowed max state = {allowed_idx}\")\n","        print(emission_vector)\n","        emissions.append(emission_vector)\n","\n","    return np.array(emissions), np.array(allowed_max_states)\n","\n","\n","\n","\n","\n","\n","def viterbi(transition_matrix, emissions, allowed_max_states):\n","    \"\"\"\n","    Modified Viterbi algorithm that, at each observation time t, only allows states\n","    up to allowed_max_states[t]. That is, for each time step the algorithm\n","    does not even consider transitions to states beyond the current allowed maximum.\n","    \"\"\"\n","    n_obs, n_states = emissions.shape\n","    # dp[t, i] holds the log probability of the most likely path ending in state i at time t.\n","    dp = np.full((n_obs, n_states), -np.inf)\n","    backpointer = np.zeros((n_obs, n_states), dtype=int)\n","\n","    for j in range(n_states):\n","        if j <= allowed_max_states[0]:\n","            dp[0, j] = emissions[0, j]\n","\n","    for t in range(1, n_obs):\n","        for j in range(n_states):\n","            if j > allowed_max_states[t]:\n","                dp[t, j] = -np.inf\n","                continue\n","\n","            max_prob = -np.inf\n","            best_state = 0\n","\n","            for i in range(n_states):\n","                if dp[t - 1, i] == -np.inf:\n","                    continue\n","                if transition_matrix[i, j] > 0:\n","                    prob = dp[t - 1, i] + np.log(transition_matrix[i, j])\n","                else:\n","                    prob = -np.inf\n","                if prob > max_prob:\n","                    max_prob = prob\n","                    best_state = i\n","            dp[t, j] = max_prob + emissions[t, j]\n","            backpointer[t, j] = best_state\n","\n","    best_last_state = np.argmax(dp[-1])\n","    state_sequence = np.zeros(n_obs, dtype=int)\n","    state_sequence[-1] = best_last_state\n","    for t in range(n_obs - 1, 0, -1):\n","        state_sequence[t - 1] = backpointer[t, state_sequence[t]]\n","    return state_sequence.tolist()\n","\n","\n","def main(midi_file):\n","    \"\"\"\n","    Main function to run the score follower.\n","\n","    1. Parse the MIDI score.\n","    2. Extract tonal and transient features from the audio.\n","    3. Compute a left-to-right transition matrix based on note durations.\n","    4. Compute emission probabilities only for states (notes) that have already started.\n","    5. Run the modified Viterbi algorithm.\n","    \"\"\"\n","    midi_notes = parse_midi(midi_file)\n","\n","    tonal, transient = extract_features()\n","\n","    sample_rate = 44100\n","\n","    E_Z_list = [int((note[1] - note[0]) * sample_rate) for note in midi_notes]\n","\n","    transition_matrix = compute_transition_matrices(E_Z_list, block_size=2048)\n","\n","    emissions, allowed_max_states = compute_emission_probabilities(\n","        tonal, transient, sample_rate, midi_notes,\n","        block_size=2048, hmm_hop_size=hmm_hop_size,\n","        T_val=T, v_val=v, sigma_n2_val=sigma_n2\n","    )\n","\n","    state_sequence = viterbi(transition_matrix, emissions, allowed_max_states)\n","\n","    print(\"Most probable state sequence:\", state_sequence)\n","    return state_sequence\n","\n","\n","\n","\n","# Run the HMM\n","midi_notes = parse_midi(midi)\n","viterbi_output = main(midi)\n","\n","\n"],"metadata":{"id":"quuIY94cjr6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_viterbi_output(midi, viterbi_output, sample_rate, hmm_hop_size)"],"metadata":{"id":"sgD4HQ4h9BdF"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1Dv4L-G9EQ73_5nTogqp0u-Uop4sktRg-","timestamp":1739550268450},{"file_id":"1hUNjGdjlmKv9ZdM81F3RXHXaO0Acnv0e","timestamp":1739309648415}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}